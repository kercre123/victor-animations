{
  "InitialVisionModes" :
  {
    // If a vision mode is not listed, it will be left in its default state
    "DetectingMarkers"           : true,
    "DetectingFaces"             : false,
    "DetectingPets"              : false,
    "DetectingMotion"            : false,
    "DetectingOverheadEdges"     : false,
    "CheckingQuality"            : true,
    "EstimatingFacialExpression" : false,
    "DetectingSmileAmount"       : false,
    "DetectingGaze"              : false,
    "DetectingBlinkAmount"       : false,
    "DetectingLaserPoints"       : false,
    "ComputingStatistics"        : false,
    "BuildingOverheadMap"        : false,
    "DetectingVisualObstacles"   : false,
    "DetectingGeneralObjects"    : false
  },
  
  "InitialModeSchedules" :
  {
    // If a vision mode is not listed, it is scheduled to run every frame (if enabled)
    // Specify either an array of bools indicating a specific schedule, or an integer
    // value to process every Nth frame
    "DetectingPets"          : [true, false],
    "DetectingOverheadEdges" : [false, true],
    "CheckingQuality"        : 15,
    "DetectingLaserPoints"   : 2,
    "ComputingStatistics"    : false,  // NOTE: enabled but not scheduled!
    "DetectingGeneralObjects": 1,
    "SavingImages"           : 5
  },
  
  "ColorImages" : true,        // Whether color images are enabled on startup (can still be toggled later)
  
  "FaceAlbum" : "robot",       // Name of album on disk to load or "robot" to use the one from the robot
  
  "FaceDetection" :
  {
    "DetectionMode": "video"   // "video" or "singleImage"
  },
  
  "FaceRecognition" :
  {
    "RunMode": "asynchronous"  // "synchronous" or "asynchronous"
  },
  
  "ImageQuality" :
  {
    // Auto-exposure settings
    "InitialExposureTime_ms"    : 16,    // Sent each time we request camera calibration
    "MidValue"                  : 115,   // [0,255] - Try to make midPercentile have this value
    "TooDarkValue"              : 20,    // "Too Dark" if HighPercentile is below this
    "TooBrightValue"            : 230,   // "Too Bright" if LowPercentile is above this
    "LowPercentile"             : 0.10,  // [0.0, 1.0]  (Near 0.0)
    "MidPercentile"             : 0.50,  // [0.0, 1.0]  (B/w low and high, higher to prefer under exposure)
    "HighPercentile"            : 0.90,  // [0.0, 1.0]  (Near 1.0)
    "MaxChangeFraction"         : 0.80,  // [0.0, 1.0]  (Amount we can change current exposure/gain each update)
    "SubSample"                 : 2,     // >= 1
    "MeterFromDetections"       : true,  // Base auto-exposure on detected markers, faces, etc, if any
    
    "TimeBeforeErrorMessage_ms" :       3000,  // How long VisionSystem must detect "bad" quality before notifying game
    "RepeatedErrorMessageInverval_ms" : 300000 // Time between error messages once triggered
  },
  
  "PerformanceLogging" :
  {
    "DropStatsWindowLength_sec"         : 30,  // How long to average dropped image stats
    "TimeBetweenProfilerInfoPrints_sec" : 60,  // How often to print Profiler info messages to the logs
    "TimeBetweenProfilerDasLogs_sec"    : 600  // How often to log Profiler DAS events
  },
  
  "PetTracker" :
  {
    "MaxPets"             : 4,    // Detectable/trackable at the same time
    "MinFaceSize"         : 60,
    "MaxFaceSize"         : 240,
    "DetectionThreshold"  : 900,  // 0 to 1000
    "InitialSearchCycle"  : 4,    // 1 to 45
    "NewSearchCycle"      : 8,    // 5 to 45
    "TrackLostCount"      : 2,    // Number of frames to continue searching for (and reporting) a lost pet
    "TrackSteadiness"     : 15    // 10 to 30 in Percentage change required to actually update size/position
  },

  "MotionDetector" :
  {
  	"HorizontalSize"    : 0.3,    // Fraction of the width of the image to be used for peripheral motion detction (right and left)
  	"VerticalSize"      : 0.4,    // Fraction of the height of the image to be used for peripheral motion detction (top)
  	"IncreaseFactor"    : 100.0,  // The higher this number, the more sensitive is motion detection to motion
  	"DecreaseFactor"    : 1.0,    // The higher this number, the more quickly motion detection forgets about motion
  	"MaxValue"          : 3.0,    // The higher this value, the sooner peripheral motion detection wilkl be triggered
    "CentroidStability" : 0.6     // Hop quickly should peripheral motion detection track the source of motion.
  },

  "OverheadMap" :
  {
    "NumRows"          : 1000,    // Size of the overhead map. Bigger sizes do not impact computation, only memory
    "NumCols"          : 1000
  },

  "GroundPlaneClassifier" :
  {
    "MaxDepth"            : 7,
    "MinSampleCount"      : 10,
    "TruncatePrunedTree"  : true,
    "Use1SERule"          : true,
    "PositiveWeight"      : 1.0,
    "OnTheFlyTrain"       : false,
    "FileOrDirName"       : "config/engine/vision/groundClassifier/deskClassifier.yaml"
  },
  
  "ObjectDetector" :
  {
    // How frequently to print or log events about timing
    "ProfilingPrintFrequency_ms"     : 10000,
    "ProfilingEventLogFrequency_ms"  : 30000,
    
    // OpenCV DNN w/ MobileNet SSD Model
    // NOTE: Currently assumes model is symlinked in resources/config/engine/vision/dnn_models to:
    //       ~/DropBox/DeepLearningVisionModels, shared by andrew (until VIC-1071)
    "graph"  : "ssd_mobilenet_v1_coco_11_06_2017_frozen.pb", 
    "labels" : "cocostuff-labels.txt", 
    "mode"   : "detection",
    "input_width" : 224, 
    "input_height": 224, 
    "input_mean_R": 0, 
    "input_mean_G": 0, 
    "input_mean_B": 0, 
    "input_std": 255, 
    "top_K": 1, 
    "min_score": 0.5

    // // SqueezeNet on ImageNet with Caffe
    // // See also: https://github.com/DeepScale/SqueezeNet
    // "graph" : "squeezenet_v1.1",
    // "labels" : "squeezenet_labels.txt",
    // "mode"   : "classification",
    // "input_width" : 227,
    // "input_height" : 227,
    // "input_mean_R" : 104, 
    // "input_mean_G" : 117,
    // "input_mean_B" : 123,
    // "input_std"    : 1,
    // "top_K" : 1,
    // "min_score" : 0.5

    // // OpencCV DNN w/ MobileNets classification model
    // //"graph"  : "mobilenet_0.50_128_RockPaperScissors_opencvdnn.pb",
    // "graph"  : "mobilenet_0.75_128_RockPaperScissors_combinedData_opencvdnn.pb",
    // "labels" : "RockPaperScissors_labels.txt",
    // "mode"   : "classification",
    // "input_width"  : 128,
    // "input_height" : 128,
    // "do_crop" : false,
    // "input_mean_R" : 127.5,
    // "input_mean_G" : 127.5,
    // "input_mean_B" : 127.5,
    // "input_std"    : 127.5,
    // "input_layer"  : "input",
    // "output_scores_layer" : "final_result",
    // "top_K" : 1,
    // "min_score" : 0.1

    // "graph"                       : "ssd_mobilenet_v1_coco_11_06_2017_quantized.pb",
    // //config["graph"] = "ssd_inception_v2_coco_11_06_2017_frozen.pb";
    // //config["graph"] = "rfcn_resnet101_coco_11_06_2017_frozen.pb"; // Doesn't work: needs Op "Round"
    // //config["graph"] = "faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017_frozen.pb"; // Doesn't work: needs Op "FloorMod"
    // //config["graph"] = "faster_rcnn_resnet101_coco_11_06_2017_frozen.pb"; // Doesn't work: needs Op "Round"
    // "labels"                      : "cocostuff-labels-no-numbers.txt",
    // "mode"                        : "detection",
    // "input_layer"                 :  "image_tensor",
    // "input_width"                 : 0,
    // "input_height"                : 0,
    // "do_crop"                     : true,
    // "output_scores_layer"         : "detection_scores",
    // "output_classes_layer"        : "detection_classes",
    // "output_boxes_layer"          : "detection_boxes",
    // "output_num_detections_layer" :  "num_detections",
    // "top_K"                       : 5,
    // "min_score"                   : 0.5
    
    //"graph"         : "inception_v3_2016_08_28_frozen.pb",
    //"labels"        : "imagenet_slim_labels.txt",
    //"input_width"   : 299,
    //"input_height"  : 299,
    //"do_crop"       : true,
    //"input_mean_R"  : 0,
    //"input_mean_G"  : 0,
    //"input_mean_B"  : 0,
    //"input_std"     : 255,
    //"input_layer"   : "input",
    //"output_layer"  : "InceptionV3/Predictions/Reshape_1",
    //"top_K"         : 1  // Only 1 supported for now

  }
}
